{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPTもぐもぐ勉強会第4回\n",
        "毎週水曜19時~20時に、ChatGPTを、API含めて、Google ColabでもOpenAI上でも夕食でも食べながらもぐもぐ勉強できればと思います...！！！  \n",
        "\n"
      ],
      "metadata": {
        "id": "YKgBZdL8gG0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section0. GPT best practices - OpenAI API\n",
        "[GPT best practices](https://platform.openai.com/docs/guides/gpt-best-practices)  が提唱する戦略は、以下の6つ\n",
        "  \n",
        "\n",
        "*   明確な指示を書く\n",
        "*   参考テキストを提供する\n",
        "*   複雑なタスクをよりシンプルなサブタスクに分割する\n",
        "*   GPTに考える時間を与える\n",
        "*   外部ツールを利用する\n",
        "*   パフォーマンステストを実施する"
      ],
      "metadata": {
        "id": "PHpmnHSYDq_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section0-1. 明確な指示を書く\n",
        "関連性の高い答えを得るには、ユーザーは文章を入力する際に重要な情報を正しく提供する必要があります。例えば「大統領は誰ですか？」という質問よりも、「2021年のメキシコ大統領は誰ですか？選挙はどのくらいの頻度で行われますか？」という質問の方が、ユーザーの意図が明確にGPTへ伝わります。\n",
        "\n",
        "また、三重引用符やXMLタグなどの区切り文字を使うと、GPTが文字をどのように扱えばいいかが分かりやすくなります。例えば「論文を要約してタイトルを付けてください。タイトル：ここにタイトルを入力、要約：ここに要約を入力」といった質問のように、ユーザーの意図がどこにあり、GPTは答えをどのように出力すればいいのかを教えてあげることが大切。タスクが複雑であればあるほど、明確に指示することが重要になります。\n",
        "\n",
        "加えて、具体的な手順を指定することも役立つとのこと。例えば「ステップ1、ユーザーが三重引用符で囲った文章を提供するので、この文章に『概要：』という接頭辞を付けて一文にまとめてください」「ステップ2、ステップ1の概要をスペイン語に翻訳し、『翻訳：』という接頭辞を付けてください」といった感じの指示です。このほか、「約50語で要約してください」「3つの箇条書きで要約してください」など、具体的な数字を提示して指示するのも有効です"
      ],
      "metadata": {
        "id": "RjCMf2Nn9nei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section0-2. ChatGPT利用時の注意点\n",
        "\n",
        "\n",
        "*   **計算は、不得手**  \n",
        "ChatGPTは大規模言語モデル(LLM)であり、数値計算や論理演算のためのアルゴリズムを内包している、内蔵しているわけではないです。したがって、桁数の大きな四則演算も誤った回答をしますし、ましてや中高校生が学ぶような数学の問題も解くことはできません。\n",
        "*   **論理的な推論が、できない**  \n",
        "ChtaGPTと会話を繰り返していると、論理的な推論を行っているように一見するとみえますが、そうではありません。LLMとしての膨大な文章や言葉から最適と思われる\"つながり\"を組み立てて、あたかも論理的に推論しているかのように振る舞っているだけです。したがって、論理パズルのような質問に、全て答えることはできません。\n",
        "*   **回答が安全、とは限らない**  \n",
        "ChatGPTにおいて、GPT-3.5では犯罪に用いられるような危険な情報も含まれている(※[ウィズセキュア、ChatGPTのサイバー攻撃への悪用の可能性をリサーチ](https://prtimes.jp/main/html/rd/p/000000350.000001340.html))ようです。しかし、プロンプトでその情報を聞いても、ChatGPTは回答しないよう設計されているようです。ただし、プロンプトを工夫することによって、危険な情報を抽出することができてしまうこともあるため、プロンプト作成には十二分に気を付ける必要があります。\n",
        "*   **機密情報を入力、してはいけない**  \n",
        "入力したプロンプトの一部に、他のユーザーへの回答の参考や回答そのものに使われてしまうリスクがあります。もし機密情報を入力してしまうと、ChatGPTの学習データ経由で機密情報が流出してしまう可能性があります。社外秘や個人情報を入力することは危険ですので、十二分に気をつけてください。\n",
        "*   **検索エンジンの代替とは、ならない**  \n",
        "GPT-3.5においては、学習された情報、データが2021年までとなるため最新の情報を訪ねてもChatGPTは回答できません。\n",
        "\n"
      ],
      "metadata": {
        "id": "tyMFZPLXH6fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section1. ChatGPT 公式リファレンスを読み解く"
      ],
      "metadata": {
        "id": "vRJqUt168DGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section1-1. GPTモデル\n",
        "OpenAI の GPT (生成事前トレーニング済みトランスフォーマー) モデルは、自然言語とコードを理解できるようにトレーニングされています。GPT は、入力に応じてテキスト出力を提供します。GPT への入力は「プロンプト」とも呼ばれます。プロンプトの設計は基本的に、GPT モデルを「プログラム」する方法であり、通常はタスクを正常に完了するための手順や例を提供します。  \n",
        "\n",
        "GPT を使用すると、次の目的でアプリケーションを構築できます。  \n",
        "\n",
        "*   **文書草案**  \n",
        "*   **コンピューターコードを書く**  \n",
        "*   **ナレッジベースに関する質問に答える**  \n",
        "*   **テキストを分析する**  \n",
        "*   **会話型エージェントを作成する**  \n",
        "*   **ソフトウェアに自然言語インターフェースを与える**  \n",
        "*   **さまざまな科目の講師**  \n",
        "*   **言語を翻訳する**  \n",
        "*   **ゲームのキャラクターをシミュレートする**  \n",
        "\n",
        "...その他にもたくさんあります！\n",
        "\n",
        "OpenAI API 経由で GPT モデルを使用するには、入力と API キーを含むリクエストを送信し、モデルの出力を含む応答を受け取ります。最新モデル`gpt-4`および `gpt-3.5-turbo`には、チャット完了 API エンドポイントを通じてアクセスします。現在、完了 API エンドポイント経由で利用できるのは古いレガシー モデルのみです。\n",
        "\n",
        "\n",
        "|  | モデルファミリー\t| APIエンドポイント |\n",
        "| --- | --- | --- |\n",
        "| 新しいモデル (2023 年以降) |gpt-4、gpt-3.5-ターボ | https://api.openai.com/v1/chat/completions |\n",
        "| ベースモデルの更新 (2023)\t| バベッジ-002、ダヴィンチ-002 | https://api.openai.com/v1/completions |\n",
        "| 従来のモデル (2020 ～ 2022 年) |text-davinci-003、text-davinci-002、ダヴィンチ、キュリー、バベッジ、エイダ | https://api.openai.com/v1/completions |\n",
        "\n",
        "[プレイグラウンド](https://platform.openai.com/playground?mode=chat)で GPT を試すことができます。どのモデルを使用すればよいかわからない場合は、`gpt-3.5-turbo` または `gpt-4` を使用してください。\n",
        "\n"
      ],
      "metadata": {
        "id": "bllVSQct8K0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section1-2. Chat completions API\n",
        "チャット モデルはメッセージのリストを入力として受け取り、モデルが生成したメッセージを出力として返します。チャット形式は複数ターンの会話を簡単にするように設計されていますが、会話のない 1 ターンのタスクにも同様に役立ちます。\n",
        "\n",
        "API 呼び出しの例は次のようになります。"
      ],
      "metadata": {
        "id": "rk9DVddkGrUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsy-gk_0Du7h",
        "outputId": "1102de3e-417d-403c-d56b-df384946906f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apikey"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbnTbHsXFCbW",
        "outputId": "9950c171-b073-413b-da2a-de9b887c47db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apikey\n",
            "  Downloading apikey-0.2.4.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: apikey\n",
            "  Building wheel for apikey (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apikey: filename=apikey-0.2.4-py3-none-any.whl size=6671 sha256=6a3098997901b58767a49902ae8e97c7a7c19629211844f3c94d775330063aee\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/b2/c9/a4400b26c52c13f16c796d15694407a8c610a3098b9e886651\n",
            "Successfully built apikey\n",
            "Installing collected packages: apikey\n",
            "Successfully installed apikey-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from apikey import APIKEY\n",
        "\n",
        "openai.api_key=APIKEY\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvzGGk2-9DNf",
        "outputId": "960255dc-7eff-46ee-e3fc-d261593b12c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7qbfFERzyoBqM4rsiwl9eoj1TXcxY at 0x7eb1bd62f330> JSON: {\n",
              "  \"id\": \"chatcmpl-7qbfFERzyoBqM4rsiwl9eoj1TXcxY\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1692772193,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 53,\n",
              "    \"completion_tokens\": 17,\n",
              "    \"total_tokens\": 70\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "完全な API リファレンス ドキュメントを参照してください。\n",
        "\n",
        "主な入力はメッセージパラメータです。メッセージはメッセージ オブジェクトの配列である必要があり、各オブジェクトには役割 (「システム」、「ユーザー」、「アシスタント」のいずれか) とコンテンツがあります。会話は 1 つのメッセージのように短くても、何度もやり取りを繰り返すこともあります。\n",
        "\n",
        "通常、会話は最初にシステム メッセージでフォーマットされ、次にユーザー メッセージとアシスタント メッセージが交互に続きます。\n",
        "\n",
        "システム メッセージは、アシスタントの動作を設定するのに役立ちます。たとえば、アシスタントの性格を変更したり、会話全体でアシスタントがどのように動作するかについて具体的な指示を提供したりできます。ただし、システム メッセージはオプションであり、システム メッセージがない場合のモデルの動作は、「あなたは役に立つアシスタントです」などの一般的なメッセージを使用する場合と同様になる可能性があることに注意してください。\n",
        "\n",
        "ユーザー メッセージは、アシスタントが応答するためのリクエストまたはコメントを提供します。アシスタント メッセージには、以前のアシスタントの応答が保存されていますが、望ましい動作の例を示すためにユーザーが作成することもできます。\n",
        "\n",
        "ユーザーの指示で以前のメッセージが参照されている場合、会話履歴を含めることが重要です。上の例では、ユーザーの最後の質問「どこで再生されましたか?」これは、2020 年のワールド シリーズに関する以前のメッセージのコンテキストでのみ意味を成します。モデルには過去のリクエストの記憶がないため、すべての関連情報が各リクエストの会話履歴の一部として提供される必要があります。会話がモデルのトークン制限内に収まらない場合は、何らかの方法で会話を短縮する必要があります。\n",
        "\n",
        "****  \n",
        "API 応答の例は次のようになります。"
      ],
      "metadata": {
        "id": "VML15LLlG7zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "{\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"finish_reason\": \"stop\",\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n",
        "        \"role\": \"assistant\"\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"created\": 1677664795,\n",
        "  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
        "  \"model\": \"gpt-3.5-turbo-0613\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"usage\": {\n",
        "    \"completion_tokens\": 17,\n",
        "    \"prompt_tokens\": 57,\n",
        "    \"total_tokens\": 74\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x48UxR_yFeI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python では、アシスタントの応答を で抽出できますresponse['choices'][0]['message']['content']。\n",
        "\n",
        "すべての応答には が含まれますfinish_reason。可能な値は次のfinish_reasonとおりです。\n",
        "\n",
        "stop: API が完全なメッセージを返した、またはstopパラメーターを介して提供された停止シーケンスの 1 つによって終了したメッセージ\n",
        "lengthmax_tokens:パラメーターまたはトークンの制限により不完全なモデル出力\n",
        "function_call: モデルは関数を呼び出すことを決定しました\n",
        "content_filter: コンテンツ フィルターのフラグにより​​省略されたコンテンツ\n",
        "null: API 応答がまだ進行中か不完全です\n",
        "入力パラメーター (以下に示す関数の提供など) に応じて、モデル応答には異なる情報が含まれる場合があります。\n",
        "\n"
      ],
      "metadata": {
        "id": "SCjypUZH95tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section1-3. 関数呼び出し\n",
        "API 呼び出しでは、gpt-3.5-turbo-0613およびに関数を記述しgpt-4-0613、それらの関数を呼び出すための引数を含む JSON オブジェクトの出力をモデルにインテリジェントに選択させることができます。チャット完了 API はこの関数を呼び出しません。代わりに、モデルはコード内で関数を呼び出すために使用できる JSON を生成します。\n",
        "\n",
        "最新のモデル (gpt-3.5-turbo-0613およびgpt-4-0613) は、(入力に応じて) 関数をいつ呼び出す必要があるかを検出し、関数の署名に準拠した JSON で応答するように微調整されています。この機能には潜在的なリスクも伴います。ユーザーに代わって世界に影響を与えるアクション (電子メールの送信、オンラインでの投稿、購入など) を実行する前に、ユーザー確認フローを組み込むことを強くお勧めします。"
      ],
      "metadata": {
        "id": "yyPzcQoC98fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "関数呼び出しを使用すると、モデルから構造化データをより確実に取得できます。たとえば、次のことができます。\n",
        "\n",
        "外部 API (ChatGPT プラグインなど) を呼び出して質問に答えるチャットボットを作成する\n",
        "たとえば、 のような関数を定義するsend_email(to: string, body: string)か、get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')\n",
        "自然言語を API 呼び出しに変換する\n",
        "例: 「私のトップ顧客は誰ですか?」を変換します。get_customers(min_revenue: int, created_before: string, limit: int)内部 API にアクセスして呼び出します\n",
        "テキストから構造化データを抽出する\n",
        "たとえば、 という関数を定義するextract_data(name: string, birthday: string)か、sql_query(query: string)\n",
        "...その他にもたくさんあります！\n",
        "\n",
        "1.   ユーザークエリと関数パラメーターで定義された一連の関数を使用してモデルを呼び出します。  \n",
        "2.   モデルは関数を呼び出すことを選択できます。その場合、コンテンツはカスタム スキーマに準拠した文字列化された JSON オブジェクトになります (注: モデルは無効な JSON を生成したり、パラメータを幻覚したりする可能性があります)。  \n",
        "3.   コード内で文字列を JSON に解析し、指定された引数が存在する場合は、それを使用して関数を呼び出します。  \n",
        "4.   関数の応答を新しいメッセージとして追加してモデルを再度呼び出し、モデルが結果を要約してユーザーに返します。  \n",
        "\n",
        "以下の例で、これらのステップの実際の動作を確認できます。"
      ],
      "metadata": {
        "id": "uBM__aq6cZBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": \"72\",\n",
        "        \"unit\": unit,\n",
        "        \"forecast\": [\"sunny\", \"windy\"],\n",
        "    }\n",
        "    return json.dumps(weather_info)\n",
        "\n",
        "\n",
        "def run_conversation():\n",
        "    # Step 1: send the conversation and available functions to GPT\n",
        "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "\n",
        "    # Step 2: check if GPT wanted to call a function\n",
        "    if response_message.get(\"function_call\"):\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"get_current_weather\": get_current_weather,\n",
        "        }  # only one function in this example, but you can have multiple\n",
        "        function_name = response_message[\"function_call\"][\"name\"]\n",
        "        fuction_to_call = available_functions[function_name]\n",
        "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
        "        function_response = fuction_to_call(\n",
        "            location=function_args.get(\"location\"),\n",
        "            unit=function_args.get(\"unit\"),\n",
        "        )\n",
        "\n",
        "        # Step 4: send the info on the function call and function response to GPT\n",
        "        messages.append(response_message)  # extend conversation with assistant's reply\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )  # extend conversation with function response\n",
        "        second_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0613\",\n",
        "            messages=messages,\n",
        "        )  # get a new response from GPT where it can see the function response\n",
        "        return second_response\n",
        "\n",
        "\n",
        "print(run_conversation())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98S8jVEDE53h",
        "outputId": "9f746c7a-6340-40f7-a08e-602bfd24c85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-7qbgHqljndkm20STz9kdfGOzk0Dr8\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1692772257,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"The weather in Boston is currently sunny and windy with a temperature of 72 degrees.\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 68,\n",
            "    \"completion_tokens\": 17,\n",
            "    \"total_tokens\": 85\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の例では、関数の応答をモデルに送り返し、モデルに次のステップを決定させました。ボストンの気温をユーザーに伝えるメッセージで応答しましたが、クエリによっては、関数を再度呼び出すことを選択する場合があります。\n",
        "\n",
        "たとえば、モデルに「今週末のボストンの天気を調べて、土曜日に 2 人分のディナーを予約して、カレンダーを更新してください」と質問し、これらのクエリに対応する関数を提供すると、モデルはそれらを連続して、次の時間にのみ呼び出すことを選択する可能性があります。最後に、ユーザー向けのメッセージを作成します。\n",
        "\n",
        "モデルに特定の関数を強制的に呼び出させたい場合は、 を設定することで実行できますfunction_call: {\"name\": \"<insert-function-name>\"}。を設定することで、モデルにユーザー向けメッセージを生成するように強制することもできますfunction_call: \"none\"。デフォルトの動作 ( function_call: \"auto\") では、関数を呼び出すかどうか、呼び出す場合はどの関数を呼び出すかをモデルが独自に決定することに注意してください。\n",
        "\n",
        "関数呼び出しのその他の例は、OpenAI クックブックで見つけることができます。"
      ],
      "metadata": {
        "id": "jJX-Ik_xgrX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section1-4. Chat completions vs. Completions\n",
        "チャットの完了形式は、単一のユーザー メッセージを使用してリクエストを作成することによって、完了形式と同様にすることができます。たとえば、次の補完プロンプトを使用して英語からフランス語に翻訳できます。\n",
        "\n",
        "```\n",
        "Translate the following English text to French: \"{text}\"\n",
        "```\n",
        "\n",
        "同等のチャット プロンプトは次のようになります。  \n",
        "\n",
        "```\n",
        "[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]\n",
        "```\n",
        "\n",
        "同様に、補完 API を使用すると、入力を適切にフォーマットすることで、ユーザーとアシスタントの間のチャットをシミュレートできます。\n",
        "\n",
        "これらの API の違いは主に、それぞれで使用できる基礎となる GPT モデルに由来します。gpt-4チャット完了 API は、最も有能なモデル ( ) および最もコスト効率の高いモデル ( gpt-3.5-turbo)へのインターフェイスです。参考までに、gpt-3.5-turboと同様の機能レベルでパフォーマンスを発揮しますが、text-davinci-003トークンあたりの価格は 10% です。価格の詳細については、こちらをご覧ください。\n"
      ],
      "metadata": {
        "id": "t0ZETNrHgutF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section2. ChatGPT エンドポイント"
      ],
      "metadata": {
        "id": "F-6qjsvSiIfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section2-1. Audio\n"
      ],
      "metadata": {
        "id": "03VMUlPUiP5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section2-1-1. 文字起こしを作成する\n",
        "`POST https://api.openai.com/v1/audio/transcriptions`  \n",
        "音声をテキストに文字起こしします。"
      ],
      "metadata": {
        "id": "vUri7BYEm4_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# リクエスト例\n",
        "!curl https://api.openai.com/v1/audio/transcriptions \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: multipart/form-data\" \\\n",
        "  -F file=\"@/path/to/file/audio.mp3\" \\\n",
        "  -F model=\"whisper-1\""
      ],
      "metadata": {
        "id": "2tkJ-vCiE6MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Request body**  \n",
        "*   **`file`** *`file`* `Required`  \n",
        "flac、mp3、mp4、mpeg、mpga、m4a、ogg、wav、または webm のいずれかの形式で、転写するオーディオ ファイル オブジェクト (ファイル名ではありません)。\n",
        "\n",
        "*   **`model`** *`string`* `Required`  \n",
        "使用するモデルのID。現在のみwhisper-1ご利用いただけます。\n",
        "\n",
        "*   **`prompt`** *`string`* `Optional`  \n",
        "モデルのスタイルをガイドしたり、前のオーディオ セグメントを継続したりするためのオプションのテキスト。プロンプトは音声言語と一致する必要があります。  \n",
        "\n",
        "*   **`response_format`** *`string`* `Optional` デフォルトは json  \n",
        "トランスクリプト出力の形式。json、text、srt、verbose_json、vtt のいずれかのオプションで指定します。\n",
        "\n",
        "*   **`temperature`** *`number`* `Optional` デフォルトは0  \n",
        "サンプリング温度。0 から 1 の間です。0.8 のような高い値は出力をよりランダムにし、0.2 のような低い値はより焦点を絞った決定的なものにします。0 に設定すると、モデルは対数確率を使用して、特定のしきい値に達するまで温度を自動的に上昇させます。\n",
        "\n",
        "*   **`language`** *`string`* `Optional`  \n",
        "入力音声の言語。入力言語をISO-639-1形式で指定すると、精度と遅延が改善されます。\n",
        "\n",
        "Returns  \n",
        "文字起こしされた、テキスト"
      ],
      "metadata": {
        "id": "n-xyzBm1nHx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section2-1-2. 翻訳の作成\n",
        "\n",
        "`POST https://api.openai.com/v1/audio/translations`\n",
        "\n",
        "音声を英語に翻訳します。"
      ],
      "metadata": {
        "id": "0Sk2vjTVpnM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# リクエスト例\n",
        "!curl https://api.openai.com/v1/audio/translations \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: multipart/form-data\" \\\n",
        "  -F file=\"@/path/to/file/german.m4a\" \\\n",
        "  -F model=\"whisper-1\""
      ],
      "metadata": {
        "id": "88hmtIViptfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Request body**  \n",
        "*   **`file`** *`file`* `Required`  \n",
        "flac、mp3、mp4、mpeg、mpga、m4a、ogg、wav、または webm のいずれかの形式で、転写するオーディオ ファイル オブジェクト (ファイル名ではありません)。\n",
        "\n",
        "*   **`model`** *`string`* `Required`  \n",
        "使用するモデルのID。現在のみwhisper-1ご利用いただけます。\n",
        "\n",
        "*   **`prompt`** *`string`* `Optional`  \n",
        "モデルのスタイルをガイドしたり、前のオーディオ セグメントを継続したりするためのオプションのテキスト。プロンプトは音声言語と一致する必要があります。  \n",
        "\n",
        "*   **`response_format`** *`string`* `Optional` デフォルトは json  \n",
        "トランスクリプト出力の形式。json、text、srt、verbose_json、vtt のいずれかのオプションで指定します。\n",
        "\n",
        "*   **`temperature`** *`number`* `Optional` デフォルトは0  \n",
        "サンプリング温度。0 から 1 の間です。0.8 のような高い値は出力をよりランダムにし、0.2 のような低い値はより焦点を絞った決定的なものにします。0 に設定すると、モデルは対数確率を使用して、特定のしきい値に達するまで温度を自動的に上昇させます。\n",
        "\n",
        "Returns  \n",
        "翻訳されたテキスト"
      ],
      "metadata": {
        "id": "YowCzpxsxDGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section2-2. Chat"
      ],
      "metadata": {
        "id": "XiVijzc8xFwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section2-2-1. チャット応答完了オブジェクト\n",
        "\n",
        "提供された入力に基づいて、モデルによって返されるチャット完了応答を表します。"
      ],
      "metadata": {
        "id": "GxUBeq2yxMej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "{\n",
        "  \"id\": \"chatcmpl-123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1677652288,\n",
        "  \"model\": \"gpt-3.5-turbo-0613\",\n",
        "  \"choices\": [{\n",
        "    \"index\": 0,\n",
        "    \"message\": {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
        "    },\n",
        "    \"finish_reason\": \"stop\"\n",
        "  }],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 9,\n",
        "    \"completion_tokens\": 12,\n",
        "    \"total_tokens\": 21\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gDJnvkG-xF8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Request body**  \n",
        "*   **`id`** *`string`*  \n",
        "チャット完了の一意の識別子。\n",
        "\n",
        "*   **`object`** *`string`*  \n",
        "オブジェクト タイプ。常に chat.completionです。\n",
        "\n",
        "*   **`created`** *`integer`*  \n",
        "チャット完了が作成されたときの UNIXな時刻。  \n",
        "\n",
        "*   **`model`** *`string`*    \n",
        "チャット完了に使用されるモデル。\n",
        "\n",
        "*   **`choices`** *`array`*  \n",
        "チャット完了の選択肢のリスト。nが1 より大きい場合は複数にすることができます。\n",
        "  *   **`index`** *`integer`*  \n",
        "  選択肢のリスト内の選択肢のインデックス。\n",
        "\n",
        "  *   **`message`** *`object`*\n",
        "  モデルによって生成されたチャット完了メッセージ。\n",
        "  \n",
        "       *   **`role`** *`string`*  \n",
        "       このメッセージの作成者の役割。\n",
        "\n",
        "       *   **`content`** *`string`* or *`null`*\n",
        "       メッセージの内容。\n",
        "\n",
        "       *   **`function_call`** *`object`*  \n",
        "       モデルによって生成された、呼び出される関数の名前と引数。\n",
        "\n",
        "         *   **`name`** *`string`*  \n",
        "         呼び出す関数の名前。\n",
        "\n",
        "         *   **`arguments`** *`string`*  \n",
        "         モデルによって JSON 形式で生成された、関数を呼び出すための引数。モデルは常に有効な JSON を生成するとは限らず、関数スキーマで定義されていないパラメーターが幻覚を示す可能性があることに注意してください。関数を呼び出す前に、コード内の引数を検証してください。\n",
        "\n",
        "   *   **`finish_reason`** *`string`*  \n",
        "   モデルがトークンの生成を停止した理由。これは、stopモデルが自然な停止ポイントまたは指定された停止シーケンスに到達した場合、 lengthリクエストで指定されたトークンの最大数に達した場合、またはfunction_callモデルが関数を呼び出した場合です。\n",
        "\n",
        "*   **`usage`** *`object`*   \n",
        "完了リクエストの使用統計。\n",
        "\n",
        "   *   **`prompt_tokens`** *`integer`*  \n",
        "   プロンプト内のトークンの数。\n",
        "\n",
        "   *   **`completion_tokens`** *`integer`*  \n",
        "   生成されたコンプリート内のトークンの数。\n",
        "\n",
        "   *   **`total_tokens`** *`integer`*  \n",
        "   リクエストで使用されたトークンの総数 (prompt + completion)。\n"
      ],
      "metadata": {
        "id": "RJ6inhO9xGMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section2-2-2. チャット完了チャンク オブジェクト\n",
        "提供された入力に基づいて、モデルによって返されたチャット完了応答のストリーミングされたチャンクを表します。"
      ],
      "metadata": {
        "id": "wpNFog1u3CCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "{\n",
        "  \"id\": \"chatcmpl-123\",\n",
        "  \"object\": \"chat.completion.chunk\",\n",
        "  \"created\": 1677652288,\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"choices\": [{\n",
        "    \"index\": 0,\n",
        "    \"delta\": {\n",
        "      \"content\": \"Hello\",\n",
        "    },\n",
        "    \"finish_reason\": \"stop\"\n",
        "  }]\n",
        "}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VlpFatAr3NWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Request body**   \n",
        "*   **`id`** *`string`*  \n",
        "チャット完了の一意の識別子。\n",
        "\n",
        "*   **`object`** *`string`*  \n",
        "オブジェクト タイプ。常に chat.completionです。\n",
        "\n",
        "*   **`created`** *`integer`*  \n",
        "チャット完了が作成されたときの UNIXな時刻。  \n",
        "\n",
        "*   **`model`** *`string`*    \n",
        "チャット完了に使用されるモデル。\n",
        "\n",
        "*   **`choices`** *`array`*  \n",
        "チャット完了の選択肢のリスト。nが1 より大きい場合は複数にすることができます。\n",
        "\n",
        "  *   **`index`** *`integer`*  \n",
        "  選択肢のリスト内の選択肢のインデックス。\n",
        "\n",
        "  *   **`delta`** *`object`*\n",
        "  ストリーミングされたモデル応答によって生成されたチャット完了デルタ。\n",
        "\n",
        "       *   **`role`** *`string`*  \n",
        "       このメッセージの作成者の役割。\n",
        "\n",
        "       *   **`content`** *`string`* or *`null`*\n",
        "       メッセージの内容。\n",
        "\n",
        "       *   **`function_call`** *`object`*  \n",
        "       モデルによって生成された、呼び出される関数の名前と引数。\n",
        "\n",
        "         *   **`name`** *`string`*  \n",
        "         呼び出す関数の名前。\n",
        "\n",
        "         *   **`arguments`** *`string`*  \n",
        "         モデルによって JSON 形式で生成された、関数を呼び出すための引数。モデルは常に有効な JSON を生成するとは限らず、関数スキーマで定義されていないパラメーターが幻覚を示す可能性があることに注意してください。関数を呼び出す前に、コード内の引数を検証してください。\n",
        "\n",
        "   *   **`finish_reason`** *`string`*  \n",
        "   モデルがトークンの生成を停止した理由。これは、stopモデルが自然な停止ポイントまたは指定された停止シーケンスに到達した場合、 lengthリクエストで指定されたトークンの最大数に達した場合、またはfunction_callモデルが関数を呼び出した場合です。"
      ],
      "metadata": {
        "id": "cD9ndv6E3Q1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section2-2-3. チャットの作成完了\n",
        "\n",
        "`POST https://api.openai.com/v1 /chat/completions`  \n",
        "指定されたチャット会話に対するモデル応答を作成します。"
      ],
      "metadata": {
        "id": "JGUctHkz3VQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello!\"\n",
        "      }\n",
        "    ]\n",
        "  }'\n"
      ],
      "metadata": {
        "id": "ogBwzejL3QQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Request body**  \n",
        "*   **`model`** *`string`* `Required`\n",
        "使用するモデルのID。Chat API で動作するモデルの詳細については、モデル エンドポイントの互換性表を参照してください。\n",
        "\n",
        "*   **`message`** *`array`* `Required`  \n",
        "これまでの会話を構成するメッセージのリスト\n",
        "\n",
        "*   `**functions**` `*array*` `Optional`  \n",
        "モデルが JSON 入力を生成する可能性がある関数のリスト。\n",
        "\n",
        "*   `**function_call**` `*string*`or`*undefined*` `Optional`  \n",
        "モデルが関数呼び出しにどのように応答するかを制御します。「none」は、モデルが関数を呼び出さず、エンドユーザーに応答することを意味します。「自動」は、モデルがエンドユーザーと関数の呼び出しのどちらかを選択できることを意味します。特定の関数を指定すると、{\"name\":\\ \"my_function\"}モデルはその関数を強制的に呼び出します。関数が存在しない場合のデフォルトは「none」です。関数が存在する場合、「auto」がデフォルトです。\n",
        "\n",
        "*   **`temperature`** *`number`*or*`null`* `Optional` デフォルトは1  \n",
        "使用するサンプリング温度 (0 ～ 2)。0.8 などの高い値では出力がよりランダムになり、0.2 などの低い値ではより集中的で確定的になります。  \n",
        "通常、これを変更するか、top_p両方を変更することは推奨しません。\n",
        "\n",
        "*   **`top_p`** *`number`*or*`null`* `Optional` デフォルトは1  \n",
        "核サンプリングと呼ばれる、温度によるサンプリングの代替方法。モデルは、top_p 確率質量を使用してトークンの結果を考慮します。したがって、0.1 は、上位 10% の確率質量を構成するトークンのみが考慮されることを意味します。  \n",
        "通常、これを変更するか、temperature両方を変更することは推奨しません。\n",
        "\n",
        "*   **`n`** *`number`*or*`null`* `Optional` デフォルトは1  \n",
        "各入力メッセージに対して生成するチャット完了の選択肢の数。\n",
        "\n",
        "*   **`stream`** *`boolean`*or*`null`* `Optional` デフォルトはFalse  \n",
        "設定すると、ChatGPT と同様に、部分的なメッセージ デルタが送信されます。トークンは、利用可能になるとデータのみのサーバー送信イベントとして送信され、ストリームはdata:\n",
        "\n",
        "*   **`stop`** *`string`*or*`array`*or*`null`* `Optional` デフォルトはnull  \n",
        "API がさらなるトークンの生成を停止する最大 4 つのシーケンス。\n",
        "\n",
        "*   **`max_tokens`** *`integer`* `Optional`  \n",
        "チャット完了で生成するトークンの最大数。  \n",
        "入力トークンと生成されたトークンの合計の長さは、モデルのコンテキストの長さによって制限されます。\n",
        "\n",
        "*   **`presence_penalty`** *`number`*or*`null`* `Optional` デフォルトは0  \n",
        "-2.0 から 2.0 までの数値。正の値を指定すると、これまでにテキストに出現したかどうかに基づいて新しいトークンにペナルティが課され、モデルが新しいトピックについて話す可能性が高まります。\n",
        "\n",
        "*   **`frequency_penalty`** *`number`*or*`null`* `Optional` デフォルトは0  \n",
        "-2.0 から 2.0 までの数値。正の値を指定すると、これまでのテキスト内の既存の頻度に基づいて新しいトークンにペナルティが課され、モデルが同じ行をそのまま繰り返す可能性が低くなります。\n",
        "\n",
        "*   **`logit_bias`** *`map`* `Optional` デフォルトはnull  \n",
        "指定したトークンが補完に表示される可能性を変更します。  \n",
        "トークン (トークナイザーのトークン ID で指定) を -100 から 100 までの関連するバイアス値にマップする json オブジェクトを受け入れます。数学的には、バイアスはサンプリング前にモデルによって生成されたロジットに追加されます。正確な効果はモデルごとに異なりますが、-1 から 1 までの値では選択の可能性が減少または増加します。-100 や 100 のような値は、関連するトークンの禁止または排他的選択をもたらすはずです。\n",
        "\n",
        "*   **`user`** *`string`* `Optional`  \n",
        "エンドユーザーを表す一意の識別子。OpenAI による不正行為の監視と検出に役立ちます。\n",
        "\n",
        "Returns  \n",
        "チャット完了オブジェクト、またはリクエストがストリーミングされている場合はチャット完了チャンクオブジェクトのストリーミングされたシーケンスを返します。"
      ],
      "metadata": {
        "id": "_RP0XY2w3hdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上となります、次回もAPIについて詳細を見ていきたいと思います！"
      ],
      "metadata": {
        "id": "BvS3n2Z7FZd6"
      }
    }
  ]
}